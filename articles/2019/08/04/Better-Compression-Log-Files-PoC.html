<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Better Compression of Log Files (PoC)</title>
  <meta name="description" content="The logs present several patterns that are repeated again and again;LZMA takes advantage of that and reaches very high compress ratios.Doing a quick test, LZ...">

  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  

  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: {inlineMath: [["$$","$$"],["\\(","\\)"]]},
	TeX: {
	  Macros: {
            
	  }
	}
      });
    </script>
    
      <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js' async></script>
    
  

  
    <script
       src="https://code.jquery.com/jquery-3.4.1.min.js"
       integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
       crossorigin="anonymous"></script>
  

  

    
      <script src='https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.9.1/underscore-min.js' ></script>
    

    
      <script src="https://d3js.org/d3.v4.min.js"></script>
    

    <script src='/book-of-gehn/js/venn/venn.min.js'></script>
    <script src='/book-of-gehn/js/venn/helper.js'></script>

    <script src='/book-of-gehn/js/fix_syntax_highlight.js'></script>
  <link rel="stylesheet" type="text/css" href="/book-of-gehn/css/tufte.css">
  <link rel="stylesheet" type="text/css" href="/book-of-gehn/css/latex.css">

  <link rel="canonical" href="/book-of-gehn/articles/2019/08/04/Better-Compression-Log-Files-PoC.html">

  <link rel="stylesheet" type="text/css" href="/book-of-gehn/css/all.min.css">

  <link type="application/atom+xml" rel="alternate" href="/book-of-gehn/feed.xml" title="The Book of Gehn" />
</head>

  <body>
    <header>
	
		<h1 class="header-title"><a href="/book-of-gehn/">The Book of Gehn</a></h1>
		
		
	

    

    
</header>

    <article class="group">
      <h1>Better Compression of Log Files (PoC)</h1>
<p class="subtitle">August 4, 2019</p>

<p>The logs present several patterns that are repeated again and again;
LZMA takes advantage of that and reaches very high compress ratios.</p>

<p>Doing a quick test, LZMA at the 6 level of compression, compressed
a 2.5 GB log into 147 MB very tight binary blog. A ratio of 94.069%,
not bad!</p>

<p>But could we get better results?
<!--more--></p>

<p>Consider the following log:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>2019-07-23T07:18:23.034218+00:00 host evaluator: info No evaluator for 4b7c9f29-f945-3641-e737-39c180263f85
2019-07-23T07:18:23.041248+00:00 host evaluator: info Submitted: 4b7c9f29-f945-3641-e737-39c180263f85
2019-07-23T07:18:23.041453+00:00 host evaluator: info Acknowledged: 4b7c9f29-f945-3641-e737-39c180263f85
2019-07-23T07:18:23.042580+00:00 host storage: info Processing 4b7c9f29-f945-3641-e737-39c180263f85
2019-07-23T07:18:23.119849+00:00 host exporter: info Sending message
2019-07-23T07:18:23.120344+00:00 host storage: info Finished processing 4b7c9f29-f945-3641-e737-39c180263f85 (0.07786840550879322s)
2019-07-23T07:18:23.132928+00:00 host exporter: error lib: Could not create socket: Too many open files
2019-07-23T07:18:23.133107+00:00 host exporter: error Exception caught: Errno::EMFILE:Too many open files - getaddrinfo
2019-07-23T07:18:23.133280+00:00 host exporter: error exporter.rb:110:in `connect'
2019-07-23T07:18:23.133427+00:00 host exporter: error exporter.rb:110:in `initialize_socket'
</code></pre></div></div>

<p>The date times have a <em>lot</em> of redundancy that a standard compressor may
not compress.</p>

<p>The substring <code class="highlighter-rouge">2019-07-23T07:18:23</code> (date and time) is repeated
several times and can be compressed but the <em>microseconds</em> part isnâ€™t.</p>

<p>So here is my plan:</p>
<ul>
  <li>split the log file in two streams: date times on the one hand and texts
on the other</li>
  <li><em>delta encode</em> the date times</li>
  <li>compress separately both streams using LZMA</li>
</ul>

<p>The proof of concept is in <a href="https://github.com/eldipa/zlog">zlog repository</a>.</p>

<p>The results? The new ratio is 96.715%, the new compressed file is 44.615%
smaller than the former <em>straight</em> LZMA compressed file.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Compression</th>
      <th style="text-align: center">Size (bytes)</th>
      <th style="text-align: center">Compression Ratio</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">None</td>
      <td style="text-align: center">2586369892</td>
      <td style="text-align: center">0</td>
    </tr>
    <tr>
      <td style="text-align: center">LZMA</td>
      <td style="text-align: center">153390408</td>
      <td style="text-align: center">94.069%</td>
    </tr>
    <tr>
      <td style="text-align: center"><em>Split</em> LZMA</td>
      <td style="text-align: center">60047300 + 24907828</td>
      <td style="text-align: center">96.715%</td>
    </tr>
  </tbody>
</table>

<h3 id="open-questions">Open questions</h3>

<p>The compressed date times represent the 41.480% of the total. The current
implementation encodes the deltas in 8 bytes and compresses the stream
using LZMA which may not be the best tool for this.</p>

<p>8 bytes perhaps is too much: if there is a log line each hour we can
represent the delta in microseconds using only 32 bits.</p>

<p>We could also pack them instead of compress them using
<a href="https://github.com/lemire/FrameOfReference">frames of reference</a>.
With a little of extra code, this would open the opportunity to
do searches by time without decompressing the whole thing.</p>

<p>LZMA and others are very good compressing repeated substrings
that are <em>closer</em> each other.</p>

<p>This is perfect of strings that represent ids in the logs
that appear in consecutive lines like
<code class="highlighter-rouge">4b7c9f29-f945-3641-e737-39c180263f85</code></p>

<p>But what about substrings that are repeated everywhere in
a <em>time independent</em>?</p>

<p>Think in <code class="highlighter-rouge">host evaluator: info No evaluator for</code>. It is very
likely to be repeated several times but if its <em>frequency</em> is
too low, several <em>other</em> lines could appear between one repetition
and the other which may confuse and reduce the performance
of the compressor.</p>

<p>Clustering the lines should bring them closer but if the operation
is not invertible without meta data (like the
<a href="https://en.wikipedia.org/wiki/Burrows%E2%80%93Wheeler_transform">Burrows-Wheeler transform</a>)
it may not be worthy.</p>

<p>Also, a clustering will go against of the
natural clustering of the <em>time dependent</em> strings. Not good.</p>



    </article>
    <span class="print-footer">Better Compression of Log Files (PoC) - August 4, 2019 - Gehn</span>
    <footer>
    <hr class="slender">
    <div class="credits">
        <span>&copy; 2021
            
            Gehn
        </span></br>
            <a style="text-decoration: none;" href="/book-of-gehn/feed.xml"><img height="16px" width="16px" src="/book-of-gehn/assets/blog-assets/rss-32px.png" /></a>
        <br>
        

    
    </div>
</footer>

  </body>
</html>
