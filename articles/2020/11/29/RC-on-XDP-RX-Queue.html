<!DOCTYPE html>
<html>
  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>RC-on-XDP-RX-Queue</title>
  <meta name="description" content="⊕  Picture this: you’d been developing for six months a network snifferusing XDP, a kernel in-pass in Linux.Six months and when you are about to release it, ...">

  
    <link href='//fonts.googleapis.com/css?family=Lato:400,400italic' rel='stylesheet' type='text/css'>
  

  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: {inlineMath: [["$$","$$"],["\\(","\\)"]]},
	TeX: {
	  Macros: {
            
	  }
	}
      });
    </script>
    
      <script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js' async></script>
    
  

  
    <script
       src="https://code.jquery.com/jquery-3.4.1.min.js"
       integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo="
       crossorigin="anonymous"></script>
  

  

    
      <script src='https://cdnjs.cloudflare.com/ajax/libs/underscore.js/1.9.1/underscore-min.js' ></script>
    

    
      <script src="https://d3js.org/d3.v4.min.js"></script>
    

    <script src='/book-of-gehn/js/venn/venn.min.js'></script>
    <script src='/book-of-gehn/js/venn/helper.js'></script>

    <script src='/book-of-gehn/js/fix_syntax_highlight.js'></script>
  <link rel="stylesheet" type="text/css" href="/book-of-gehn/css/tufte.css">
  <link rel="stylesheet" type="text/css" href="/book-of-gehn/css/latex.css">

  <link rel="canonical" href="/book-of-gehn/articles/2020/11/29/RC-on-XDP-RX-Queue.html">

  <link rel="stylesheet" type="text/css" href="/book-of-gehn/css/all.min.css">

  <link type="application/atom+xml" rel="alternate" href="/book-of-gehn/feed.xml" title="The Book of Gehn" />
</head>

  <body>
    <header>
	
		<h1 class="header-title"><a href="/book-of-gehn/">The Book of Gehn</a></h1>
		
		
	

    

    
</header>

    <article class="group">
      <h1>RC-on-XDP-RX-Queue</h1>
<p class="subtitle">November 29, 2020</p>

<p><label for="mf-482d5a7a07746c31570f73c75afe4260" class="margin-toggle  in-index-only">⊕</label><input type="checkbox" id="mf-482d5a7a07746c31570f73c75afe4260" class="margin-toggle  in-index-only" /><span class="marginnote  in-index-only"><img style="max-width:120%;" alt="RC on RX queue" src="/book-of-gehn/assets/xdp-rx-rc/rx-rc-queues.png" />  <br /></span></p>

<p>Picture this: you’d been developing for six months a network sniffer
using XDP, a kernel <em>in-pass</em> in Linux.</p>

<p>Six months and when you are about to release it, you find not one but
three bugs that shake all your understanding of XDP.</p>

<p>A debugging race against the clock begins.<!--more--></p>

<p>We were hitting three issues in a row:</p>

<ul>
  <li>RX queue returns addresses with the incorrect offset</li>
  <li>More packets hold by the application than possible</li>
  <li>Pointer to <code class="highlighter-rouge">NULL</code> data</li>
</ul>

<p>All of them at random times but very often.</p>

<h2 id="xdp-rx-queue">XDP RX queue</h2>

<p>The XDP RX queue is a lock free single-producer, single-consumer queue
where the kernel plays the role of the producer and the user
application the consumer.</p>

<p>The kernel pushes addresses (offsets respect the UMEM’s base address)
into the queue that points to the received packets.</p>

<p>The <em>pop</em> has three parts:</p>

<ul>
  <li>the user application calls <code class="highlighter-rouge">xsk_ring_cons__peek</code> to know how many
packets are ready to be consumed.</li>
  <li>then, for each one a call to <code class="highlighter-rouge">xsk_ring_cons__rx_desc</code> to get the
packet’s descriptor and therefore, its address (<code class="highlighter-rouge">addr</code> field)</li>
  <li>and finally a call to <code class="highlighter-rouge">xsk_ring_cons__release</code> to mark the descriptors
free to be reused by the producer.</li>
</ul>

<p>There is no need to <em>process</em> the packets before <code class="highlighter-rouge">xsk_ring_cons__release</code>:
releasing the packets’ descriptors of the RX does not make the UMEM’s
frames holding the packets free to be reused.</p>

<p>Only when the packets’ addresses are pushed into the fill queue (FQ)
the frames are available again.</p>

<figure><figcaption><span>Addresses of free frames are consumed from the FQ queue by the kernel (1).
XDP then writes the incoming packet in a free frame (2) and pushes the
address into the RX queue (3). The user application consumes from the RX
queue (4) addresses of packets ready to be read or write (5). When it is
done, the address is pushed back to the FQ for reuse (6).</span></figcaption><img style="max-width:80%;" alt="UMEM cycle" src="/book-of-gehn/assets/xdp-rx-rc/rx-rc-umem-cycle.png" /></figure>

<p>The packet descriptor returned by <code class="highlighter-rouge">xsk_ring_cons__rx_desc</code> has two
attributes: the packet’s address and its length.</p>

<p>The address is an <code class="highlighter-rouge">uint64_t</code> offset respect the UMEM’s base address.</p>

<p><label for="mf-e34702a9b002fa12328e1ec2277e184c" class="margin-toggle ">⊕</label><input type="checkbox" id="mf-e34702a9b002fa12328e1ec2277e184c" class="margin-toggle " /><span class="marginnote "><img class="fullwidth" alt="UMEM frame" src="/book-of-gehn/assets/xdp-rx-rc/rx-rc-frame.png" />  <br /></span></p>

<p>The UMEM is a memory pool divided evenly by 2048 or 4096 bytes, the
frame size and addresses are aligned to the frame size plus an offset for a
headroom.</p>

<p>The headroom is an application defined space reserved at the begin of
the frame for whatever the user wants to do. By default it’s zero.</p>

<p>Well, for some reason the RX queue was returning sometimes addresses
with the wrong offset.</p>

<h2 id="more-than-possible">More than possible</h2>

<p>The UMEM is a fixed memory pool divided evenly in frames where each
frame holds a packet.</p>

<p>Therefore the count of packets that the application can hold
before releasing them is fixed (UMEM size / frame size).</p>

<p>At anytime the count is less than or equal to UMEM size / frame size.</p>

<p>However the counters of the application shown a different thing: more
packets where entering in the application and were hold before releasing
them than the expected!</p>

<h2 id="pointer-to-null">Pointer to <code class="highlighter-rouge">NULL</code></h2>

<p>As mentioned before you can prepend metadata to each packet.</p>

<p>In our case, one of the attributes is a pointer to an external
structure and the pointer is never updated again. Remains constant.</p>

<p>But to our surprise the pointer changes and leaves pointing to <code class="highlighter-rouge">NULL</code>.</p>

<p>And the code is extraordinary simple and straightforward so there is
no chances to change the pointer to <code class="highlighter-rouge">NULL</code> by an error in the logic.</p>

<div class="language-cpp highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">void</span> <span class="nf">do_work</span><span class="p">(</span><span class="kt">void</span> <span class="o">*</span><span class="n">ctx</span><span class="p">)</span> <span class="p">{</span>
  <span class="k">while</span> <span class="p">(</span><span class="n">alive</span><span class="p">)</span> <span class="p">{</span>
    <span class="k">struct</span> <span class="n">pkt_t</span> <span class="o">*</span><span class="n">pkt</span> <span class="o">=</span> <span class="n">read_packet</span><span class="p">();</span>  <span class="c1">// pull from the RX queue</span>

    <span class="n">pkt</span><span class="o">-&gt;</span><span class="n">obj</span> <span class="o">=</span> <span class="n">external_object</span><span class="p">();</span>
    <span class="n">assert</span> <span class="p">(</span><span class="n">pkt</span><span class="o">-&gt;</span><span class="n">obj</span><span class="p">);</span>                 <span class="c1">// not NULL</span>

    <span class="cm">/* moments later */</span>
    <span class="n">use</span><span class="p">(</span><span class="n">pkt</span><span class="o">-&gt;</span><span class="n">obj</span><span class="o">-&gt;</span><span class="n">field</span><span class="p">);</span>   <span class="c1">// segmentation fault, pkt-&gt;obj is NULL</span>

    <span class="n">free_packet</span><span class="p">(</span><span class="n">pkt</span><span class="p">);</span>       <span class="c1">// push into the FQ queue</span>
  <span class="p">}</span>
<span class="p">}</span>
</code></pre></div></div>

<p>No chances.</p>

<h2 id="debugging">Debugging</h2>

<p>So far we have three unrelated bugs. While possible, it is unlikely that
we are dealing with three <em>independent</em> bugs.</p>

<p>They must be related somehow.</p>

<p>Let’s spin a debugger.</p>

<h3 id="counting-before-a-crash">Counting before a crash</h3>

<p>How many packets were pulled from the RX queue before getting one crash.</p>

<pre><code class="language-gdb">(gdb) b read_packet
(gdb) ignore 1 1000000
</code></pre>

<p>When the bug is detected, GDB will stop and we’ll have the chance to see
how many times the breakpoint was hit before.</p>

<pre><code class="language-gdb">(gdb) info break
&lt;...&gt;breakpoint already hit 29 times
</code></pre>

<p>Would this change in function of the UMEM size? Larger UMEMs has more
frames.</p>

<p>These are the results:</p>

<pre><code class="language-gdb">// Minimum size, UMEM can hold 1 frame only
breakpoint already hit 29 times
breakpoint already hit 19 times
breakpoint already hit 23 times

// Slightly larger UMEM, it can hold 16 frames
breakpoint already hit 69 times
breakpoint already hit 43 times
breakpoint already hit 50 times

// 64 frames
breakpoint already hit 111 times
breakpoint already hit 69 times
breakpoint already hit 126 times

// 256 frames
breakpoint already hit 348 times
breakpoint already hit 324 times
breakpoint already hit 1 time
</code></pre>

<p>So, with larger UMEMs is less likely to hit the bug but it is not a hard
rule. In the last test, with the largest UMEM, we hit the bug in the
first try.</p>

<h3 id="spontaneous-addresses">Spontaneous addresses</h3>

<p>What about the addresses returned by the RX queue? We want to print them
without stopping the process.</p>

<pre><code class="language-gdb">(gdb) b do_work.c:16     // after the call to read_packet()
(gdb) commands
&gt; silent
&gt; p pkt - umem-&gt;base
&gt; cont
&gt; end
</code></pre>

<p>The segmentation fault due the <code class="highlighter-rouge">NULL</code> pointer happen even when the
addresses had the correct offsets (<code class="highlighter-rouge">pkt - umem-&gt;base % frame_size ==
headroom_size</code>)</p>

<p>For the ones with incorrect offset, the address most common was the 0
(<code class="highlighter-rouge">pkt - umem-&gt;base == 0</code>).</p>

<p>I hypothesized that I could be putting the address 0 by mistake in the
FQ. Further testing shown that the 0 was never put in FQ but still being
received in the RX queue.</p>

<p>So the RX queue was returning addresses that I never put in the FQ.
Those are new addresses!</p>

<p>That explains the issue number 2: the application having more packets
than it should.</p>

<p>Later, I found that 0 was not the only addresses with an incorrect
offset (issue number 1).</p>

<h3 id="poison-values">Poison values</h3>

<p>Let’s print the packets as soon as they are read and the external object
is set:</p>

<pre><code class="language-gdb">(gdb) set print pretty on

(gdb) b do_work.c:19     // after the call to external_object()
(gdb) commands
&gt; silent
&gt; p *pkt
&gt; cont
&gt; end
</code></pre>

<p>The last packet printed before the segmentation fault (issue 3)
was something like this</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  timestamp = 121212121,
  length = 60,
  data = 0xsomeaddress
}
</code></pre></div></div>

<p>And after the crash, the same packet looked like this:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  timestamp = 0,
  length = 60,
  data = 0x0
}
</code></pre></div></div>

<p>So the <code class="highlighter-rouge">timestamp</code> and <code class="highlighter-rouge">data</code> where zero’d. Was this done by the
application or something else happen?</p>

<p>Perhaps a rouge <code class="highlighter-rouge">memset</code>?</p>

<p>The fields are initialized to zero by the eBPF filter and overwritten by
the application.</p>

<p>I decided to set them to non-trivial values, called <em>poison values</em>.</p>

<p>If a <code class="highlighter-rouge">memset</code> zero’d them, I will notice.</p>

<p>This is the packet <strong>after</strong> the crash:</p>

<p><label for="mmkd-2d4ae39ec8ffcd63da15ac50ce0f3e16" class="margin-toggle"> ⊕</label><input type="checkbox" id="mmkd-2d4ae39ec8ffcd63da15ac50ce0f3e16" class="margin-toggle" /></p>
<div id="mk-mmkd-2d4ae39ec8ffcd63da15ac50ce0f3e16"><span class="marginnote marginmarkdowncode"><pre><code class="language-gdb">(gdb) p/x pkt-&gt;timestamp
0xdead
</code></pre>If you didn&apos;t realize, the timestamp was not zero&apos;d either.</span></div>
<div><script>$(document).ready(function () {$('#mk-mmkd-2d4ae39ec8ffcd63da15ac50ce0f3e16 > span').insertAfter($('#mmkd-2d4ae39ec8ffcd63da15ac50ce0f3e16'))});</script></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
  timestamp = 57005,
  length = 60,
  data = 0xbeaf
}
</code></pre></div></div>

<p>So the whole structure was <em>not zero’d but reset</em>, overwritten by
the eBPF filter when the packet was supposed to be managed by user.</p>

<p>The three bugs are symptoms of the same unknown problem: the RX queue is
returning invalid addresses, not only without the expected offset but
addresses that belong to packets that the kernel still thinks that are
free.</p>

<h2 id="the-real-bug">The real bug</h2>

<p>Something was wrong in the RX queue / kernel side so we started to
search this issue in the web.</p>

<p>A college of mine found a candidate: the fix of a race condition in the
generic receive path.</p>

<p>eBPF runs in the driver if this one supports it. If not, eBPF is
executed in the kernel and the packets take a slightly larger path from
the network card to user.</p>

<p>This path is known as the XDP generic path or just XDP generic.</p>

<p>Remember than the RX is a single-producer queue so it is not thread safe
for concurrent pushes.</p>

<p><label for="mf-482d5a7a07746c31570f73c75afe4260" class="margin-toggle ">⊕</label><input type="checkbox" id="mf-482d5a7a07746c31570f73c75afe4260" class="margin-toggle " /><span class="marginnote "><img style="max-width:120%;" alt="RC on RX queue" src="/book-of-gehn/assets/xdp-rx-rc/rx-rc-queues.png" />  <br /></span></p>

<p>No problem when XDP runs in the driver but when it does in the generic
mode, the kernel may be receiving several packets and pushing them
<strong>concurrently</strong> into the RX queue.</p>

<blockquote>
  <p>“Unlike driver mode, generic xdp receive could be triggered
by different threads on different CPU cores at the same time
leading to the fill and rx queue breakage. For example, this
could happen while sending packets from two processes to the
first interface of <code class="highlighter-rouge">veth</code> pair while the second part of it is
open with <code class="highlighter-rouge">AF_XDP</code> socket.</p>

  <p>Need to take a lock for each generic receive to avoid race.”
<cite class="epigraph"><a href="https://github.com/torvalds/linux/commit/bf0bdd1343efbbf65b4d53aef1fce14acbd79d50">commit bf0bdd13</a></cite></p>
</blockquote>

<p>The commit fixes the issue
using a <code class="highlighter-rouge">spinlock</code> but the fix was not backported.</p>

<p>And doing a kernel upgrade is <strong>not an option</strong>.</p>

<h2 id="single-queue">Single queue</h2>

<p><label for="mn-3f29fa60e45a9506f034bbefc74e782a" class="margin-toggle"> ⊕</label><input type="checkbox" id="mn-3f29fa60e45a9506f034bbefc74e782a" class="margin-toggle" /><span class="marginnote">The article
<a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a>
explains this wonderfully.
<br />
Despite the name, the articule describes all the network stack from the
driver to a TCP/UDP socket.
 </span></p>

<p>Once a packet is received by the network card a interruption is
signaled. The interruption runs briefly and delegates the rest of the
processing to a kernel thread named <code class="highlighter-rouge">ksoftirqd/n</code>.</p>

<p>The packet is put by the interruption into a queue to be consumed
by a <strong>single</strong>  <code class="highlighter-rouge">ksoftirqd/n</code> thread.</p>

<p>How the kernel can process multiple packet in parallel?</p>

<p>With multiple queues of course!</p>

<p>If we cannot upgrade the kernel we must enforce a single producer thread
in the kernel side.</p>

<p>Configuring the interface to use a <strong>single</strong> RX queue the kernel
will use a <strong>single</strong> <code class="highlighter-rouge">ksoftirqd/n</code> thread, a single-producer.</p>

<p><label for="mn-b547aee63f894013ca6bf9c81f006fac" class="margin-toggle"> ⊕</label><input type="checkbox" id="mn-b547aee63f894013ca6bf9c81f006fac" class="margin-toggle" /><span class="marginnote">Try <code class="highlighter-rouge">sudo ethtool -L &lt;iface&gt; combined 1</code> if the other does not work.
 </span></p>

<p>Thankfully the configuration is one liner:</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo </span>ethtool <span class="nt">-L</span> &lt;iface&gt; rx 1
</code></pre></div></div>

<h2 id="conclusions">Conclusions</h2>

<p>This was hard. One innocently expects the bugs in the user application,
not in the kernel.</p>

<p>And most of the time that’s true!</p>

<p>Debugging confirmed the opposite. And it was not easy.</p>

<p>The <code class="highlighter-rouge">do_work</code> shown is an oversimplification. The real code decouple the
<code class="highlighter-rouge">read_packet</code> from the processing from the <code class="highlighter-rouge">free_packet</code> into a serie
of threads.</p>

<p>And if debugging a multithreading application is not hard enough, putting a
breakpoint in some places added enough delay that the bug was not
trigger anymore.</p>

<p><a href="https://en.wikipedia.org/wiki/Heisenbug">Heisenbug!</a></p>

<p>GDB’s <code class="highlighter-rouge">set non-stop on</code> helped to reduce the impact: when a breakpoint
is hit by a thread, only that thread is stopped.</p>

<p>A special thanks to my college Mario that dug into kernel’s git log and
found <a href="https://github.com/torvalds/linux/commit/bf0bdd1343efbbf65b4d53aef1fce14acbd79d50">commit bf0bdd13</a>.</p>

<p>That was the missing piece to solve this puzzle.</p>

<h2 id="references">References</h2>

<ul>
  <li><a href="https://blog.packagecloud.io/eng/2016/06/22/monitoring-tuning-linux-networking-stack-receiving-data/">Monitoring and Tuning the Linux Networking Stack: Receiving Data</a></li>
  <li><a href="https://www.infradead.org/~mchehab/rst_conversion/networking/scaling.html">Scaling in the Linux Networking Stack</a></li>
  <li><a href="https://www.kernel.org/doc/html/latest/networking/af_xdp.html">Linux AF_XDP</a></li>
</ul>




    </article>
    <span class="print-footer">RC-on-XDP-RX-Queue - November 29, 2020 - Gehn</span>
    <footer>
    <hr class="slender">
    <div class="credits">
        <span>&copy; 2021
            
            Gehn
        </span></br>
            <a style="text-decoration: none;" href="/book-of-gehn/feed.xml"><img height="16px" width="16px" src="/book-of-gehn/assets/blog-assets/rss-32px.png" /></a>
        <br>
        

    
    </div>
</footer>

  </body>
</html>
